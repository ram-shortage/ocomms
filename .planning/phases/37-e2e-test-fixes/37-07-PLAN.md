---
phase: 37-e2e-test-fixes
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/api/export-data/route.ts
  - scripts/reset-and-seed.ts
  - scripts/seed.ts
  - scripts/e2e-seed.ts
  - src/server/socket/handlers/message.ts
  - src/workers/link-preview.worker.ts
  - scripts/restore.sh
autonomous: true

must_haves:
  truths:
    - "Export API filters DMs by organization membership"
    - "Destructive scripts have production safety gates"
    - "E2E seed is idempotent with proper scoping"
  artifacts:
    - path: "src/app/api/export-data/route.ts"
      provides: "Secure data export"
    - path: "scripts/reset-and-seed.ts"
      provides: "Safe reset script"
---

<objective>
Fix code review findings from CODE_REVIEW_06.MD.

Purpose: Address security vulnerabilities, data leakage, and infrastructure robustness issues identified in the code review. These must be fixed before the test suite can be considered reliable.

Output: All HIGH and MEDIUM severity issues resolved.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@scripts/CODE_REVIEW_06.MD
@src/app/api/export-data/route.ts
@scripts/reset-and-seed.ts
@scripts/e2e-seed.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: [HIGH] Fix Cross-Organization Data Leakage in Exports</name>
  <files>
    src/app/api/export-data/route.ts
  </files>
  <action>
**Issue:** Notifications are filtered with `channelIds.includes(n.channelId!) || n.conversationId`, which includes ANY DM notification regardless of org membership. Users in multiple orgs can export notifications from other workspaces.

**Location:** `export-data.ts:204-213`

**Fix:**
1. Build a set of conversation IDs belonging to the target organization
2. Filter DM notifications by those conversation IDs
3. Or join against `conversations` table with `organizationId` filter

```typescript
// Before (VULNERABLE):
const notifications = allNotifications.filter(n =>
  channelIds.includes(n.channelId!) || n.conversationId
);

// After (SECURE):
// Get conversation IDs for this org
const orgConversations = await db.query.conversations.findMany({
  where: eq(conversations.organizationId, organizationId),
  columns: { id: true }
});
const orgConversationIds = new Set(orgConversations.map(c => c.id));

const notifications = allNotifications.filter(n =>
  channelIds.includes(n.channelId!) ||
  (n.conversationId && orgConversationIds.has(n.conversationId))
);
```
  </action>
  <verify>
- Export only includes notifications from the specified organization
- DM notifications from other workspaces are excluded
- Write a test case to verify cross-org leakage is prevented
  </verify>
  <done>
Export API properly scopes DM notifications to organization
  </done>
</task>

<task type="auto">
  <name>Task 2: [HIGH] Add Production Safety Gates to Destructive Scripts</name>
  <files>
    scripts/reset-and-seed.ts
    scripts/seed.ts
    scripts/e2e-seed.ts
  </files>
  <action>
**Issue:** `reset-and-seed.ts` drops all tables with only a 3-second delay. Scripts insert test users with hard-coded passwords. If run against production, this destroys data and creates predictable accounts.

**Location:** `reset-and-seed.ts:11-28`, `seed.ts:26-117`, `e2e-seed.ts:20-80`

**Fix:**
1. Add production URL detection (check DATABASE_URL for production patterns)
2. Require explicit `ALLOW_DESTRUCTIVE=true` environment variable
3. Add `NODE_ENV !== 'production'` check
4. Follow pattern used in `reset-demo.ts`

```typescript
// Add at top of destructive scripts:
const isProduction = process.env.NODE_ENV === 'production' ||
  process.env.DATABASE_URL?.includes('prod') ||
  process.env.DATABASE_URL?.includes('railway') ||
  process.env.DATABASE_URL?.includes('supabase');

if (isProduction && process.env.ALLOW_DESTRUCTIVE !== 'true') {
  console.error('❌ REFUSED: Cannot run destructive script in production');
  console.error('   Set ALLOW_DESTRUCTIVE=true to override (DANGEROUS)');
  process.exit(1);
}

// For seed scripts with test credentials:
if (isProduction) {
  console.error('❌ REFUSED: Cannot seed test data in production');
  process.exit(1);
}
```
  </action>
  <verify>
- Scripts refuse to run when DATABASE_URL contains production indicators
- Scripts refuse to run when NODE_ENV=production
- ALLOW_DESTRUCTIVE override works for legitimate use cases
- Test by setting DATABASE_URL to a fake production URL and verifying refusal
  </verify>
  <done>
All destructive scripts have production safety gates
  </done>
</task>

<task type="auto">
  <name>Task 3: [MEDIUM] Fix E2E Seed Membership Logic Idempotency</name>
  <files>
    scripts/e2e-seed.ts
  </files>
  <action>
**Issue:** Member existence check is scoped only by `userId` (not `organizationId`), so users in any org skip membership creation for target org. Channel membership check uses `limit(1)` incorrectly.

**Location:** `e2e-seed.ts:158-226`

**Fix:**
1. Scope member checks by both `userId` AND `organizationId`
2. Use `onConflictDoNothing` with composite unique key for inserts
3. Fix channel membership check to query by both `channelId` AND `userId`

```typescript
// Check org membership properly:
const existingMember = await db.query.member.findFirst({
  where: and(
    eq(member.userId, userId),
    eq(member.organizationId, organizationId)
  )
});

// Use onConflictDoNothing for idempotent inserts:
await db.insert(member)
  .values({ userId, organizationId, role: 'member' })
  .onConflictDoNothing();

// Fix channel membership check:
const existingChannelMember = await db.query.channelMember.findFirst({
  where: and(
    eq(channelMember.channelId, channelId),
    eq(channelMember.userId, userId)
  )
});
```
  </action>
  <verify>
- Running e2e-seed.ts twice produces same result (idempotent)
- No duplicate key errors on rerun
- Users are correctly added to specified organization
  </verify>
  <done>
E2E seed is idempotent with proper org/channel scoping
  </done>
</task>

<task type="auto">
  <name>Task 4: [MEDIUM] Upgrade Socket Rate Limiting to Redis</name>
  <files>
    src/server/socket/handlers/message.ts
  </files>
  <action>
**Issue:** `RateLimiterMemory` allows bypass in multi-instance deployments by spreading requests across nodes.

**Location:** `src/server/socket/handlers/message.ts:90-95`

**Fix:**
1. Replace `RateLimiterMemory` with `RateLimiterRedis`
2. Use existing Redis connection from the app
3. Fallback to memory if Redis unavailable (with warning)

```typescript
import { RateLimiterRedis, RateLimiterMemory } from 'rate-limiter-flexible';
import { getRedisClient } from '@/lib/redis';

let rateLimiter: RateLimiterRedis | RateLimiterMemory;

const redisClient = getRedisClient();
if (redisClient) {
  rateLimiter = new RateLimiterRedis({
    storeClient: redisClient,
    keyPrefix: 'socket_rl',
    points: 10,
    duration: 1,
  });
} else {
  console.warn('[Socket.IO] Redis unavailable, using in-memory rate limiting');
  rateLimiter = new RateLimiterMemory({
    points: 10,
    duration: 1,
  });
}
```
  </action>
  <verify>
- Rate limiting uses Redis when available
- Fallback to memory works without Redis
- Rate limits are shared across server instances
  </verify>
  <done>
Socket rate limiting uses Redis for multi-instance consistency
  </done>
</task>

<task type="auto">
  <name>Task 5: [MEDIUM] Add Response Size Limit to Link Preview Fetches</name>
  <files>
    src/workers/link-preview.worker.ts
  </files>
  <action>
**Issue:** `node-fetch` has no `size` limit, so very large responses can exhaust memory during unfurling.

**Location:** `src/workers/link-preview.worker.ts:59-71`, `118-122`

**Fix:**
1. Add `size` option to fetch calls (e.g., 5MB max)
2. Consider streaming with early abort for large responses
3. Add stricter timeout for large bodies

```typescript
const MAX_RESPONSE_SIZE = 5 * 1024 * 1024; // 5MB

const response = await fetch(url, {
  signal: AbortSignal.timeout(10000),
  size: MAX_RESPONSE_SIZE,
  headers: {
    'User-Agent': 'OComms Link Preview Bot',
  },
});

// Or with streaming abort:
const response = await fetch(url, { signal: AbortSignal.timeout(10000) });
const contentLength = response.headers.get('content-length');
if (contentLength && parseInt(contentLength) > MAX_RESPONSE_SIZE) {
  throw new Error('Response too large');
}
```
  </action>
  <verify>
- Large responses (>5MB) are rejected
- Memory usage stays bounded during link preview processing
- Normal-sized pages still work correctly
  </verify>
  <done>
Link preview fetches have response size limits
  </done>
</task>

<task type="auto">
  <name>Task 6: [MEDIUM] Fix Restore Script Error Handling</name>
  <files>
    scripts/restore.sh
  </files>
  <action>
**Issue:** `pg_restore ... || true` always prints "Done." even on failure, leaving DB empty or partially restored without warning.

**Location:** `restore.sh:82-85`

**Fix:**
1. Remove `|| true` and handle errors properly
2. Capture exit codes
3. Add verification step after restore

```bash
# Before:
pg_restore ... || true
echo "Done."

# After:
if ! pg_restore --verbose --clean --if-exists --no-owner \
    --dbname="$DATABASE_URL" "$BACKUP_FILE"; then
  echo "❌ ERROR: pg_restore failed!"
  exit 1
fi

# Verify restore succeeded
echo "Verifying restore..."
TABLES=$(psql "$DATABASE_URL" -t -c "SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public'")
if [ "$TABLES" -lt 1 ]; then
  echo "❌ ERROR: No tables found after restore!"
  exit 1
fi

echo "✅ Restore completed successfully ($TABLES tables)"
```
  </action>
  <verify>
- Failed restores exit with error code
- Success message only shown on actual success
- Verification query confirms tables exist
  </verify>
  <done>
Restore script properly reports errors
  </done>
</task>

<task type="auto">
  <name>Task 7: [LOW/MEDIUM] Optimize Export N+1 Queries</name>
  <files>
    src/app/api/export-data/route.ts
  </files>
  <action>
**Issue:** Export uses many N+1 queries (profiles, reactions, notifications) causing long runtimes and memory pressure for large orgs.

**Location:** `export-data.ts:78-90`, `115-129`, `164-170`, `204-214`

**Fix:**
1. Batch queries using `inArray` on IDs
2. Use joins where appropriate
3. Implement pagination for very large exports

```typescript
// Before (N+1):
for (const message of messages) {
  const reactions = await db.query.reactions.findMany({
    where: eq(reactions.messageId, message.id)
  });
}

// After (batched):
const messageIds = messages.map(m => m.id);
const allReactions = await db.query.reactions.findMany({
  where: inArray(reactions.messageId, messageIds)
});
const reactionsByMessage = groupBy(allReactions, 'messageId');
```

Priority: Lower than security fixes, but improves reliability for large workspaces.
  </action>
  <verify>
- Export query count is O(1) not O(n)
- Large workspace exports complete in reasonable time
- Memory usage stays bounded
  </verify>
  <done>
Export queries are batched for efficiency
  </done>
</task>

</tasks>

<verification>
HIGH Priority:
- [ ] Export API filters DMs by organization (no cross-org leakage)
- [ ] Destructive scripts refuse to run in production

MEDIUM Priority:
- [ ] E2E seed is idempotent (can run multiple times safely)
- [ ] Socket rate limiting uses Redis
- [ ] Link preview has response size limit
- [ ] Restore script reports errors properly

LOW/MEDIUM Priority:
- [ ] Export queries are batched (no N+1)
</verification>

<success_criteria>
- All HIGH severity issues fixed and verified
- All MEDIUM severity issues fixed
- Scripts are safe to use in CI/CD pipelines
- No data leakage vulnerabilities
</success_criteria>

<output>
After completion, create `.planning/phases/37-e2e-test-fixes/37-07-SUMMARY.md`
</output>
